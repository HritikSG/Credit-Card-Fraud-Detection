# -*- coding: utf-8 -*-
"""ECE537_DataMining_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11VN7n8-qE1mKRkGLLe49GrE6p-SLJkh6
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

data = pd.read_csv('Fraud.csv')

"""## Data Preprocessing"""

data.head()

data.columns

data.dtypes

#Convert date to datetime
data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])

#checking data distribution
data.describe

#checking null values
data.isna().sum()

# Droppping duplicates
data = data.drop_duplicates()

#taking log of amount to remove the skewness
data['log_amount']=np.log(data['amt'])

"""## Feature Engineering"""

#Creating time features for better understanding
data['hour_of_day'] = data['trans_date_trans_time'].dt.hour
data['day_of_week'] = data['trans_date_trans_time'].dt.dayofweek  # Monday=0, Sunday=6
data['month'] = data['trans_date_trans_time'].dt.month

#Calculate avg amount spent on each credit card
avg_amounts = data.groupby('cc_num')['amt'].mean().reset_index()
avg_amounts.columns = ['cc_num', 'avg_amt']

# Merge the average amounts back to the original DataFrame
data = data.merge(avg_amounts, on='cc_num')

# Calculate the deviation from the average transaction amount
data['deviation'] = data['amt'] - data['avg_amt']

data['deviation'] = data['deviation'].abs()

data.query("is_fraud == 1")

# Frequency of transactions in the last day
current_time = data['trans_date_trans_time'].max()  # Get the latest transaction time
last_day_transactions = data[data['trans_date_trans_time'] > (current_time - pd.Timedelta(days=1))]

transaction_counts_last_day = last_day_transactions.groupby('cc_num').size().reset_index(name='transaction_count_last_day')

data = data.merge(transaction_counts_last_day, on='cc_num', how='left')

#Frequency of transcation for last hour
last_hour_transactions = data[data['trans_date_trans_time'] > (current_time - pd.Timedelta(hours=1))]

transaction_counts_last_hour = last_hour_transactions.groupby('cc_num').size().reset_index(name='transaction_count_last_hour')

data = data.merge(transaction_counts_last_hour, on='cc_num', how='left')

# Function to calculate the Haversine distance
def haversine(lat1, lon1, lat2, lon2):
    # Convert degrees to radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])

    # Haversine formula
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

    # Earth radius in kilometers
    r = 6371.0
    return r * c

#Calculate distance between consecutive transactions
data['distance'] = data.apply(
    lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1
)

# Flag Night Transaction
# Define business hours (9 AM to 5 PM)
business_start = pd.Timestamp('09:00:00').time()
business_end = pd.Timestamp('17:00:00').time()

# Flag transactions outside business hours
data['is_night_transaction'] = data['trans_date_trans_time'].apply(
    lambda x: not (business_start <= x.time() <= business_end)
)

"""## Exploratory Data Analysis"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot distribution of transaction amounts
plt.figure(figsize=(10, 6))
sns.histplot(data['amt'], bins=30, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('amt')
plt.ylabel('Frequency')
plt.show()

# Plot class distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='is_fraud', data=data)
plt.title('Class Distribution: Fraud vs. Non-Fraud')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])
plt.show()

# Select features for pair plot
features = ['log_amount']
sns.pairplot(data[features + ['is_fraud']], hue='is_fraud')
plt.title('Pair Plot of Selected Features')
plt.ylabel('is_fraud')
plt.show()

# Select features for pair plot
features = ['amt']
sns.pairplot(data[features + ['is_fraud']], hue='is_fraud')
plt.title('Pair Plot of Selected Features')
plt.ylabel('is_fraud')
plt.show()

# Select features for pair plot
features = ['day_of_week']
sns.pairplot(data[features + ['is_fraud']], hue='is_fraud')
plt.title('Pair Plot of Selected Features')
plt.ylabel('is_fraud')
plt.show()

# Select features for pair plot
features = ['hour_of_day']
sns.pairplot(data[features + ['is_fraud']], hue='is_fraud')
plt.title('Pair Plot of Selected Features')
plt.ylabel('is_fraud')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='is_fraud', y='amt', data=data)
plt.title('Transaction Amounts by Class')
plt.xlabel('Class')
plt.ylabel('Transaction Amount')
plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='is_fraud', y='deviation', data=data)
plt.title('deviation Amounts by Class')
plt.xlabel('Class')
plt.ylabel('deviation Amount')
plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='is_fraud', y='distance', data=data)
plt.title('distance')
plt.xlabel('Class')
plt.ylabel('distance')
plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])
plt.show()

data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])
data.set_index('trans_date_trans_time', inplace=True)

# Resample the data to daily counts
daily_counts = data.resample('D').size()

# Plot daily transaction counts
plt.figure(figsize=(14, 6))
daily_counts.plot()
plt.title('Daily Transaction Counts')
plt.xlabel('Date')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=45)
plt.grid()
plt.show()

# Filter fraud transactions
fraud_data = data[data['is_fraud'] == 1]

#1. Fraud Distribution by Hour of the Day
plt.figure(figsize=(10, 6))
sns.countplot(x='hour_of_day', data=fraud_data, palette='viridis')
plt.title('Fraud Distribution by Hour of the Day')
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Fraud Transactions')
plt.show()

# 2. Fraud Distribution by Day of the Week
day_of_week_map = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}
fraud_data['day_of_week'] = fraud_data['day_of_week'].map(day_of_week_map)
plt.figure(figsize=(10, 6))
sns.countplot(x='day_of_week', data=fraud_data, palette='coolwarm', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.title('Fraud Distribution by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Number of Fraud Transactions')
plt.show()

# Fraud Distribution by Month of the Year
plt.figure(figsize=(10, 6))
sns.countplot(x='month', data=fraud_data, palette='magma')
plt.title('Fraud Distribution by Month of the Year')
plt.xlabel('Month of the Year')
plt.ylabel('Number of Fraud Transactions')
plt.show()

average_distance = data.groupby('is_fraud')['distance'].mean().reset_index()

average_distance['is_fraud'] = average_distance['is_fraud'].replace({0: 'Non-Fraud', 1: 'Fraud'})

# Plot the average distance for fraud and non-fraud transactions
plt.figure(figsize=(8, 6))
sns.barplot(x='is_fraud', y='distance', data=average_distance, palette='Set2')

plt.title('Average Distance for Non-Fraud vs Fraud Transactions')
plt.xlabel('Transaction Type')
plt.ylabel('Average Distance')
plt.show()

"""## Feature Selection"""

from sklearn.preprocessing import LabelEncoder


# One-Hot Encode 'job' and 'category' columns
data_encoded = pd.get_dummies(data, columns=['category'], drop_first=True)

# Label Encoding for 'gender'

# Initialize LabelEncoder
le = LabelEncoder()

# Fit and transform the 'gender' column
data_encoded['gender'] = le.fit_transform(data['gender'])

data_encoded.columns

corr_columns = ['is_fraud',
       'log_amount', 'hour_of_day', 'day_of_week', 'month', 'avg_amt',
       'deviation', 'transaction_count_last_day',
       'transaction_count_last_hour', 'distance','amt','gender']

# Heatmap
# Now run the correlation analysis
correlation_matrix = data_encoded[corr_columns].corr()

# Plot the heatmap to visualize the correlation matrix
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Entropy Calculation
def calculate_conditional_entropy(data, feature_col, target_col='is_fraud'):
    #Calculate the joint distribution of each feature value with the target
    joint_prob = data.groupby([feature_col, target_col]).size() / len(data)
    #Calculate the marginal probability of the target
    marginal_prob = data[target_col].value_counts() / len(data)
    #Calculate the conditional entropy
    conditional_entropy = 0
    for (feature_val, target_val), prob in joint_prob.items():
        if prob > 0:  #Avoid log(0) issue
            conditional_entropy += prob * np.log2(marginal_prob[target_val] / prob)
    return conditional_entropy

#Calculate conditional entropy for each column with respect to 'is_fraud' and store results
entropy_values = {col: calculate_conditional_entropy(data, col, 'is_fraud') for col in data.columns if col != 'is_fraud'}

#Sort the dictionary by entropy values in ascending order
sorted_entropy_values = dict(sorted(entropy_values.items(), key=lambda item: item[1]))

for i in range(2):
    print(f"Lowest entropy columns at Column '{list(sorted_entropy_values.keys())[i]}' has an entropy of {list(sorted_entropy_values.values())[i]}")

from sklearn.ensemble import RandomForestClassifier

# Load your preprocessed dataset
# X = features, y = target (fraud column)
num_columns =  ['is_fraud',
       'log_amount', 'hour_of_day', 'day_of_week', 'month', 'avg_amt',
       'deviation', 'distance','amt','gender']
X = data_encoded[num_columns].drop('is_fraud', axis=1)
y = data_encoded['is_fraud']

# Initialize RandomForestClassifier with optimizations
rf = RandomForestClassifier(n_estimators=20, max_depth=10, random_state=42, n_jobs=-1)

# Train the model on a sample of data (if necessary)
rf.fit(X, y)

# Get feature importance
feature_importance = rf.feature_importances_

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importance
}).sort_values(by='Importance', ascending=False)

# Print or plot feature importance as needed
print(feature_importance_df)

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Initialize a logistic regression model
model = LogisticRegression(max_iter=1000)

# Initialize RFE and fit it to the data
rfe = RFE(model, n_features_to_select=10)  # Select top 10 features
rfe = rfe.fit(X, y)

# Get the selected features
selected_features = X.columns[rfe.support_]
print("Selected Features: ", selected_features)

# Check the ranking of features
print("Feature Ranking: ", rfe.ranking_)

data_encoded[corr_columns].isna().sum()

"""## Model Training and Model Selection"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score

# Splitting the Data with Stratification
X = data_encoded[num_columns].drop('is_fraud', axis=1)  # Replace 'fraud' with your target column name
y = data_encoded['is_fraud']

# Perform stratified split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Standardize the features (Logistic regression works better with standardized data)
scaler = StandardScaler()

# Fit the scaler on the training data and transform both train and test data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression Model
log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')

# Fit the model to the training data
log_reg.fit(X_train_scaled, y_train)

# Make Predictions on the Test Data
y_pred = log_reg.predict(X_test_scaled)

# Model Evaluation
# Print Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print Accuracy Score
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_test, y_pred)
print("ROC-AUC Score:", roc_auc)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False, xticklabels=["Non-Fraud", "Fraud"], yticklabels=["Non-Fraud", "Fraud"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

